---
title: 'Lab 05: Stepwise Regression'
author: "Section 1 Magenta"
date: '2022-10-01'
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE, fig.align = "center")
```

Packages used for this lab:
```{r}
library(tidyverse)
library(GGally)
```


Importing data set:
```{r}
heart <- read_csv("heart.csv")
```


### Question 1: Make a scatter plot matrix that shows the pairwise relationships between all variables. Do the data satisfy the model assumptions of generalised linear regression?
```{r}
pairs(heart)
age_thalach <- lm(age ~ thalach, data = heart)
summary(age_thalach)
```
The above scatterplot matrix shows some linearity in the relationship between `age` and `thalach`. To double check, we perform a linear regression analysis between these variables and caculated a very low p-value of $5.63e-13$, suggesting a linear relationship between them. Hence, the assumption of absence of multicollinearity assumption is violated. 

There doesn't seem to be many outstanding outliers in the scatterplot matrix. You could say that in the plot against `age` vs `trestbps`, there is a point near the top of the plot that seems to deviate from the cluster of points, however, this isn't a big outlier as the point isn't that far from the rest of the points and thus is not a strongly influential outlier. The same can be said for other scatterplots which have points that are a bit far from the main cluster of points, however, those points still follow the general direction of the cluster of points (e.g., `age` vs `chol`).

The scatterplots also don't seem to thicken and roughly maintain a constant density of points throughout, hence, satisfying the homoscedasticity assumption. 

For the assumptions on the errors (e.g., independence of errors), we are unable to tell if the data satisfies this assumption as we have only plotted a scatter plot matrix, and not a residual plot. 

In sum, the data seems to violate the conditions of zero or little multicollinearity, and it is necessary to remove one or some predictor variable(s) to perform an approriate generalised linear regression analysis.

### Question 2: Consider logistic regression with disease as the outcome. Perform backward elimination based on p-values using a significance level of 0.001 to obtain a parsimonious model for the presence of heart disease.
```{r}
log_reg_all <- glm(disease ~ ., family = binomial, data = heart)
summary(log_reg_all)
log_reg_1 <- glm(disease ~ . - fbs, family = binomial, data = heart)
summary(log_reg_1)
log_reg_2 <- glm(disease ~ . - fbs - age, family = binomial, data = heart)
summary(log_reg_2)
log_reg_3 <- glm(disease ~ . - fbs - age - chol, family = binomial, data = heart)
summary(log_reg_3)
log_reg_4 <- glm(disease ~ . - fbs - age - chol - trestbps, family = binomial, data = heart)
summary(log_reg_4)

log_reg_be <- log_reg_4
```

### Question 3: Interpret the coeï¬€icient with the smallest p-value in the model selected in task (2). Make sure your interpretation is in context and explains the respective predictor.
```{r}
summary(log_reg_be)
```
`thalach` is the predictor variable that has the smallest p-value of $1.71e-07$, and hence, is the most statistically significant predictor variable. This means that an increase in 1 unit of the maximum heart rate achieved (`thalach`) will result in an increase of the log odds of coronary heart disease (more than 50% diameter narrowing of any major heart vessel) by $0.037298$. 

### Question 4: Create a side-by-side box plot of the predicted probabilities from your final model from task (2), split by whether the person had heart disease or not. What does this plot suggest about how well this model can determine whether somebody has heart disease?

```{r}
heart$probs <- predict(log_reg_be, type = "response")
boxplot(probs ~ disease,
  data = heart,
  xlab = "Heart Disease",
  ylab = "Predicted Probabilities",
  main = "Predicted Probabilities vs Actual Heart Disease"
)
```

The side-by-side box plots show a significant difference in the predicted probabilities of having heart disease between the two groups of people. This suggests that the model can predict the probability of a person's having heart disease quite well, since the predicted probability of having heart disease of a person who has heart disease should be closer to 1, and that of a person without heart disease should be closer to 0. The mean predicted probability of having heart disease for the group with no heart disease is approximately 0.3, while that of the other group is approximately 0.7. 

### Question 5: Repeat task (2) but now using forward selection rather than backward elimination. Again use a significance level of 0.001.
```{r}
log_reg_age <- glm(formula = disease ~ age, data = heart)
summary(log_reg_age)
log_reg_sex <- glm(formula = disease ~ sex, data = heart)
summary(log_reg_sex)
log_reg_trestbps <- glm(formula = disease ~ trestbps, data = heart)
summary(log_reg_trestbps)
log_reg_chol <- glm(formula = disease ~ chol, data = heart)
summary(log_reg_chol)
log_reg_fbs <- glm(formula = disease ~ fbs, data = heart)
summary(log_reg_fbs)
log_reg_thalach <- glm(formula = disease ~ thalach, data = heart)
summary(log_reg_thalach)
log_reg_exang <- glm(formula = disease ~ exang, data = heart)
summary(log_reg_exang)

log_reg_fs_1 <- glm(formula = disease ~ exang + thalach, data = heart)
summary(log_reg_fs_1)
log_reg_fs_2 <- glm(formula = disease ~ exang + thalach + sex, data = heart)
summary(log_reg_fs_2)
log_reg_fs_3 <- glm(formula = disease ~ exang + thalach + sex + age, data = heart)
summary(log_reg_fs_3)

log_reg_fs <- glm(formula = disease ~ exang + thalach + sex, data = heart)
```

### Question 6: Are your final models selected in tasks (2) and (5) the same? Are the final models from forward selection and backward elimination always the same? Please explain.

Yes, the final models from tasks (2) and (5) are the same. However, final models from forward and backward elimination are not always the same. Consider the case when the independent variables are correlated with each other. Then, the addition or removal of a new variable may cause variables that were already in the model to be non-significant. There is also a possibility that predictors are significant with a different and specific model configuration. So, the variables selected in forward selection may differ from that of backward elimination, leading the final models to differ. 